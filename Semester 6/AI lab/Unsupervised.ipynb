{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "471aaadb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Kmeans 1\n",
    "\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "#visulaization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set(style='darkgrid')\n",
    "#data and model\n",
    "from sklearn import datasets\n",
    "from sklearn.cluster import KMeans\n",
    "%matplotlib inline\n",
    "\n",
    "iris=datasets.load_iris()\n",
    "X=iris.data[:, :2]\n",
    "y=iris.target\n",
    "\n",
    "plt.scatter(X[:,0], X[:,1], c=y, cmap='gist_rainbow')\n",
    "plt.xlabel('Spea1 Length', fontsize=18)\n",
    "plt.ylabel('Sepal Width', fontsize=18)\n",
    "plt.title('Iris Data Distribution', fontsize=20)\n",
    "km = KMeans(n_clusters = 3,  random_state=21)\n",
    "km.fit(X)\n",
    "centers = km.cluster_centers_\n",
    "print(centers)\n",
    "new_labels = km.labels_\n",
    "plt.scatter(X[:,0], X[:,1], c=new_labels, cmap='gist_rainbow')\n",
    "plt.xlabel('Spea1 Length', fontsize=18)\n",
    "plt.ylabel('Sepal Width', fontsize=18)\n",
    "plt.title('Iris Data Distribution', fontsize=20)\n",
    "\n",
    "new_labels = km.labels_\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16,8))\n",
    "axes[0].scatter(X[:, 0], X[:, 1], c=y, cmap='gist_rainbow',\n",
    "edgecolor='k', s=150)\n",
    "axes[1].scatter(X[:, 0], X[:, 1], c=new_labels, cmap='jet',\n",
    "edgecolor='k', s=150)\n",
    "axes[0].set_xlabel('Sepal length', fontsize=18)\n",
    "axes[0].set_ylabel('Sepal width', fontsize=18)\n",
    "axes[1].set_xlabel('Sepal length', fontsize=18)\n",
    "axes[1].set_ylabel('Sepal width', fontsize=18)\n",
    "axes[0].tick_params(direction='in', length=10, width=5, colors='k', labelsize=20)\n",
    "axes[1].tick_params(direction='in', length=10, width=5, colors='k', labelsize=20)\n",
    "axes[0].set_title('Actual', fontsize=18)\n",
    "axes[1].set_title('Predicted', fontsize=18)\n",
    "\n",
    "\n",
    "# elbow method\n",
    "x=iris.data[:, :4]\n",
    "wcss=[]\n",
    "for i in range(1 , 11):\n",
    "  km=KMeans(n_clusters=i, init='k-means++', max_iter=300, n_init=10, random_state=0)\n",
    "  km.fit(x)\n",
    "  wcss.append(km.inertia_)\n",
    "\n",
    "#plotting to results to observe the elbow\n",
    "plt.plot(range(1, 11), wcss)\n",
    "plt.title('The Elbow Method')\n",
    "plt.xlabel('Number of Clusters')\n",
    "plt.ylabel('WCSS')\n",
    "plt.show()\n",
    "kms=KMeans(n_clusters = 3, init='k-means++', max_iter=300, n_init=10, random_state=0)\n",
    "ykmeans=kms.fit_predict(x)\n",
    "print(ykmeans)\n",
    "\n",
    "#plotting the centroids\n",
    "plt.scatter(x[ykmeans == 0, 0], x[ykmeans ==0, 1], s=100, c='red', label='iris-setosa')\n",
    "plt.scatter(x[ykmeans == 1, 0], x[ykmeans ==1, 1], s=100, c='blue', label='iris-versicolor')\n",
    "plt.scatter(x[ykmeans == 2, 0], x[ykmeans ==2, 1], s=100, c='green', label='iris-virginica')\n",
    "plt.scatter(kms.cluster_centers_[:, 0],kms.cluster_centers_[:, 1], s=100, c='black', label='Centroids')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "068ac0ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]]\n",
      "Score over time: 0.50075\n",
      "[[7.08694501e-02 6.52301015e-02 1.10399235e-01 7.03162208e-02]\n",
      " [2.83529346e-02 3.26858767e-02 5.22959991e-02 7.06789594e-02]\n",
      " [2.20703046e-02 2.34452801e-02 5.05068333e-02 5.43064346e-02]\n",
      " [1.04108805e-02 4.06463113e-02 1.18206237e-03 5.47047155e-02]\n",
      " [3.21960606e-01 7.23880331e-02 7.16073413e-02 3.89906002e-02]\n",
      " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
      " [3.55963069e-04 2.63556983e-04 4.74803149e-02 3.02425827e-04]\n",
      " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
      " [4.79938112e-02 6.55224881e-03 7.40831670e-02 5.93851991e-01]\n",
      " [1.17231879e-01 8.18685603e-01 5.83189002e-02 3.16395827e-03]\n",
      " [4.11992630e-01 7.59974312e-04 1.00414065e-02 5.95768577e-03]\n",
      " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
      " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
      " [1.38209241e-01 1.50601863e-02 7.84189130e-01 1.43969460e-01]\n",
      " [2.75510084e-01 9.42334058e-01 2.53458322e-01 3.87632728e-01]\n",
      " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]]\n",
      "****************************************************\n",
      "EPISODE  0\n",
      "We reached our Goal üèÜ\n",
      "Number of steps 26\n",
      "****************************************************\n",
      "EPISODE  1\n",
      "****************************************************\n",
      "EPISODE  2\n",
      "We reached our Goal üèÜ\n",
      "Number of steps 69\n",
      "****************************************************\n",
      "EPISODE  3\n",
      "****************************************************\n",
      "EPISODE  4\n",
      "We reached our Goal üèÜ\n",
      "Number of steps 7\n",
      "Action space: Discrete(4)\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'FrozenLakeEnv' object has no attribute 'nS'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-b2b43c61d529>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    121\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Action space:\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maction_space\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    122\u001b[0m \u001b[0menv1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munwrapped\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 123\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"State space:\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnS\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    124\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mP\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    125\u001b[0m \u001b[0mstate_size\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m16\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\gym\\core.py\u001b[0m in \u001b[0;36m__getattr__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m    227\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"_\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    228\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"attempted to get missing private attribute '{name}'\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 229\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0menv\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    230\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    231\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\gym\\core.py\u001b[0m in \u001b[0;36m__getattr__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m    227\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"_\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    228\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"attempted to get missing private attribute '{name}'\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 229\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0menv\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    230\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    231\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'FrozenLakeEnv' object has no attribute 'nS'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import gym\n",
    "import random\n",
    "\n",
    "env = gym.make(\"FrozenLake-v1\")\n",
    "\n",
    "action_size = env.action_space.n\n",
    "state_size = env.observation_space.n\n",
    "# Create our Q table with state_size rows and action_size columns (64x4)\n",
    "qtable = np.zeros((state_size, action_size))\n",
    "print(qtable)\n",
    "\n",
    "#Here, we'll specify the hyperparameters\n",
    "total_episodes = 20000       # Total episodes\n",
    "learning_rate = 0.7          # Learning rate\n",
    "max_steps = 99               # Max steps per episode\n",
    "gamma = 0.95                 # Discounting rate\n",
    "\n",
    "# Exploration parameters\n",
    "epsilon = 1.0                 # Exploration rate\n",
    "max_epsilon = 1.0             # Exploration probability at start\n",
    "min_epsilon = 0.01            # Minimum exploration probability \n",
    "decay_rate = 0.005            # Exponential decay rate for exploration prob\n",
    "\n",
    "#Now we implement the Q learning algorithm:\n",
    "# List of rewards\n",
    "rewards = []\n",
    "\n",
    "# 2 For life or until learning is stopped\n",
    "for episode in range(total_episodes):\n",
    "    # Reset the environment\n",
    "    state = env.reset()\n",
    "    step = 0\n",
    "    done = False\n",
    "    total_rewards = 0\n",
    "    \n",
    "    for step in range(max_steps):\n",
    "        # 3. Choose an action a in the current world state (s)\n",
    "        ## First we randomize a number\n",
    "        exp_exp_tradeoff = random.uniform(0, 1)\n",
    "        \n",
    "        ## If this number > greater than epsilon --> exploitation (taking the biggest Q value for this state)\n",
    "        if exp_exp_tradeoff > epsilon:\n",
    "            action = np.argmax(qtable[state,:])\n",
    "            #print(exp_exp_tradeoff, \"action\", action)\n",
    "\n",
    "        # Else doing a random choice --> exploration\n",
    "        else:\n",
    "            action = env.action_space.sample()\n",
    "            #print(\"action random\", action)\n",
    "            \n",
    "        \n",
    "        # Take the action (a) and observe the outcome state(s') and reward (r)\n",
    "        new_state, reward, done, info = env.step(action)\n",
    "\n",
    "        # Update Q(s,a):= Q(s,a) + lr [R(s,a) + gamma * max Q(s',a') - Q(s,a)]\n",
    "        # qtable[new_state,:] : all the actions we can take from new state\n",
    "        qtable[state, action] = qtable[state, action] + learning_rate * (reward + gamma * np.max(qtable[new_state, :]) - qtable[state, action])\n",
    "        \n",
    "        total_rewards += reward\n",
    "        \n",
    "        # Our new state is state\n",
    "        state = new_state\n",
    "        \n",
    "        # If done (if we're dead) : finish episode\n",
    "        if done == True: \n",
    "            break\n",
    "        \n",
    "    # Reduce epsilon (because we need less and less exploration)\n",
    "    epsilon = min_epsilon + (max_epsilon - min_epsilon)*np.exp(-decay_rate*episode) \n",
    "    rewards.append(total_rewards)\n",
    "    \n",
    "\n",
    "print (\"Score over time: \" +  str(sum(rewards)/total_episodes))\n",
    "print(qtable)\n",
    "\n",
    "#Use our Q-table to play FrozenLake ! üëæ\n",
    "#After 10 000 episodes, our Q-table can be used as a \"cheatsheet\" to play FrozenLake\"\n",
    "#By running this cell you can see our agent playing FrozenLake.\n",
    "\n",
    "env.reset()\n",
    "\n",
    "for episode in range(5):\n",
    "    state = env.reset()\n",
    "    step = 0\n",
    "    done = False\n",
    "    print(\"****************************************************\")\n",
    "    print(\"EPISODE \", episode)\n",
    "\n",
    "    for step in range(max_steps):\n",
    "        \n",
    "        # Take the action (index) that have the maximum expected future reward given that state\n",
    "        action = np.argmax(qtable[state,:])\n",
    "        \n",
    "        new_state, reward, done, info = env.step(action)\n",
    "        \n",
    "        if done:\n",
    "            # Here, we decide to only print the last state (to see if our agent is on the goal or fall into an hole)\n",
    "            env.render()\n",
    "            if new_state == 15:\n",
    "                print(\"We reached our Goal üèÜ\")\n",
    "            else:\n",
    "                print(\"We fell into a hole ‚ò†Ô∏è\")\n",
    "            \n",
    "            # We print the number of step it took.\n",
    "            print(\"Number of steps\", step)\n",
    "            \n",
    "            break\n",
    "        state = new_state\n",
    "env.close()\n",
    "\n",
    "#Now we implement the SARSA learning algorithm:\n",
    "import random\n",
    "\n",
    "import gym\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "env = gym.make('FrozenLake-v1')\n",
    "print(\"Action space:\", env.action_space)\n",
    "env1 = env.unwrapped\n",
    "print(\"State space:\", env.env.nS)\n",
    "print(env.env.P[0])\n",
    "state_size = 16\n",
    "action_space = env.action_space.n\n",
    "alpha = 0.1\n",
    "gamma = 1\n",
    "state_action_vals = np.random.randn(state_size, action_space)\n",
    "policy = np.zeros(state_size, dtype=int)\n",
    "episodes = 20000\n",
    "eps = 0.2\n",
    "test_episodes = 50\n",
    "test_every = 1000\n",
    "test_episode = []\n",
    "rewards = []\n",
    "def select_action(state, eps):\n",
    "    sample = np.random.uniform()\n",
    "    if sample < eps:\n",
    "        return env.action_space.sample()\n",
    "    else:\n",
    "        return state_action_vals[state].argmax()\n",
    "    \n",
    "for ep in range(episodes):\n",
    "    state = env.reset()\n",
    "    action = select_action(state, eps)\n",
    "    done = False\n",
    "    while not done:\n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "        next_action = select_action(state, eps)\n",
    "\n",
    "        action_value = state_action_vals[state, action]\n",
    "        next_action_value = state_action_vals[next_state, next_action]\n",
    "        delta = reward + gamma * next_action_value - action_value\n",
    "        state_action_vals[state, action] += alpha * delta\n",
    "        state, action = next_state, next_action\n",
    "        \n",
    "    if ep % test_every == 0:\n",
    "        total_rewards = 0\n",
    "        for _ in range(test_episodes):\n",
    "            done = False\n",
    "            state = env.reset()\n",
    "            while not done:\n",
    "                action = state_action_vals[state].argmax()\n",
    "                state, reward, done, _ = env.step(action)\n",
    "                total_rewards += reward\n",
    "        rewards.append(total_rewards / test_episodes)\n",
    "        test_episode.append(ep)\n",
    "        \n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(test_episode, rewards)\n",
    "ax.set_title('Episodes vs average rewards')\n",
    "ax.set_xlabel('Episode')\n",
    "_ = ax.set_ylabel('Average reward')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0e0ff82",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
